{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №2: линейная регрессия (10 баллов).\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считаю свой вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print((len(\"Kazakbaev\") % 4) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8281308819724393e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "mse_lin = mean_squared_error(y, reg.predict(X))\n",
    "print(mse_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8281308819724393e-25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y-reg.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.834495168221677e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-7.57619106e-09,  7.19932809e-09,  3.22765741e-08,  1.19970327e-08,\n",
       "       -5.35534458e-08,  3.79878232e+01, -2.46688805e-08, -8.35478173e-08,\n",
       "       -2.69081580e-08,  9.55179619e-09,  1.51937975e-08,  3.52771717e-08,\n",
       "       -7.42943194e-09,  9.47226298e-09,  1.64842549e-08,  3.56980964e-08,\n",
       "       -2.72165408e-08,  2.58367876e-08, -2.24285662e-08,  5.92176538e+01,\n",
       "        5.23182668e-09, -1.82645736e-09, -1.78241088e-08,  9.22875184e+01,\n",
       "        1.06139762e-08, -4.62962145e-08,  3.11046692e-08, -4.51692844e-08,\n",
       "       -2.19211855e-08, -4.09619883e-08, -2.56861772e-08, -6.79376171e-09,\n",
       "        7.41519953e+01,  6.19539814e+00,  5.64223685e-08, -2.17424283e-08,\n",
       "        8.06539928e-09,  4.03653071e-08,  2.17329518e-08,  3.78904802e-08,\n",
       "       -2.78752696e-08,  2.08864222e-08,  1.62772378e-08, -2.96220735e-08,\n",
       "       -4.05992826e-08, -2.12686709e-08,  2.81444337e-08, -1.53097581e-08,\n",
       "       -1.11333865e-08,  5.59267674e-09,  6.50919588e-09,  4.19532297e+00,\n",
       "        1.49316947e-08,  3.01452747e-08,  9.90147620e-09,  7.18788989e-09,\n",
       "        2.16601620e-08,  4.01069603e-08, -1.92008482e-09,  3.02999060e-08,\n",
       "        2.19351257e-08, -2.74540967e-08,  2.68099121e-08, -3.17402139e-08,\n",
       "        3.59144143e-08, -5.25730043e-08,  1.36084967e-08, -2.44690269e-08,\n",
       "        8.57723990e+01, -4.87484706e-09, -3.53399674e-08,  5.53957646e-08,\n",
       "       -2.85100685e-08,  4.47291479e-08, -4.71271552e-08, -1.84500640e-08,\n",
       "        7.97585408e-10, -7.17096395e-10,  5.51295602e+01,  3.10035386e-08,\n",
       "        6.12638121e+01,  2.79494324e-09, -2.35736267e-08,  1.99753648e-08,\n",
       "       -5.30791519e-08, -2.35468659e-09,  2.81414768e-08,  2.95983424e-09,\n",
       "        2.65272058e-09, -3.77179496e-08, -8.64573197e-09,  9.79776861e-09,\n",
       "        9.75525740e-09,  7.06068086e-08, -6.68764064e-08,  6.38954089e+01,\n",
       "       -7.26751462e-09,  3.50280190e-08, -2.35786953e-08, -2.81141923e-08])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся, к документации. Просто Linear Regression минимизирует квадрат разницы того, что неблюдается и то, что предсказано. В то же время, как SGDRegressor сдвигается в сторону антиградиента с определенным шагом - learning rate, и с наличием penalty. У SGDRegressor есть penalty который по дефолту стоит L2, у Linear regression этого нет. \n",
    "\n",
    "Коэффицент $\\alpha$ это множитель регулирязатора(penalty), то есть то насколько сильно мы штрафуем за слишком большие веса. Уменьшая/увеличивая $\\alpha$ будем получать разные MSE.\n",
    "\n",
    "Соответсвенно будет и другой минимизируемая функция: $Q = \\frac {1} {l} \\sum ((w, x_i) - y_i) ^ 2 + \\alpha * \\sum (w_i) ^2$\n",
    "\n",
    "И соотвественно разные MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.824314905332355e-12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это наиболее близкое, что я подбором смог сделать. Сокращая альфу, мы сокращаем MSE. В любом случае получившиеся значение это очень маленькое число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, learning_rate=0.01, l_ratio=0.001, tol=0.001, epochs=1500, weights=np.zeros(X.shape[1]),\n",
    "        costs = [0]):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        learning_rate - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.epochs = epochs  \n",
    "        self.costs = costs\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y_true = y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        weights = self.weights\n",
    "        costs = self.costs\n",
    "        learning_rate = self.learning_rate\n",
    "        N = float(len(y_true))\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = np.dot(X, weights)\n",
    "            cost = 1 / N * np.sum((y_true - y_pred)** 2)\n",
    "            weights_gradient = (-2 / N) * np.dot(X.T, (y_true - y_pred)) \n",
    "            weights = weights - learning_rate * weights_gradient\n",
    "            costs.append(cost)\n",
    "            if abs(costs[-1] - costs[-2]) < epsilon:\n",
    "                break\n",
    "        return weights\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        y_pr = np.dot(X, self.fit(X, y))\n",
    "        return y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "class LinearRegression_l2(object):\n",
    "    def __init__(self, learning_rate=0.01, l_ratio=0.1, tol=0.001, epochs=10000, weights=np.zeros(X.shape[1]),\n",
    "        costs = [0]):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        learning_rate - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.epochs = epochs  \n",
    "        self.costs = costs\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y_true = y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        weights = self.weights\n",
    "        costs = self.costs\n",
    "        learning_rate = self.learning_rate\n",
    "        l_ratio = self.l_ratio\n",
    "        N = float(len(y_true))\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = np.dot(X, weights)\n",
    "            cost = 1 / N * np.sum((y_true - y_pred)** 2) + l_ratio * np.sum(weights ** 2)\n",
    "            weights_gradient = (-2 / N) * np.dot(X.T, (y_true - y_pred)) + (l_ratio * 2 * weights) / N\n",
    "            weights = weights - learning_rate * weights_gradient\n",
    "            costs.append(cost)\n",
    "            if abs(costs[-1] - costs[-2]) < epsilon:\n",
    "                break\n",
    "        return weights\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        y_pr = np.dot(X, self.fit(X, y))\n",
    "        return y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work! even with l2 ratio\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression_l2()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work! even with l2 ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.7556571658919374e-06\n",
      "Weights: [-1.88869862e-05 -1.22093245e-05 -2.40969034e-05 -9.57000920e-06\n",
      " -2.02034603e-05  3.79874415e+01  4.48751964e-06 -2.57362717e-05\n",
      "  4.15801608e-06  1.67372682e-05  2.76461837e-06  1.29957328e-05\n",
      " -1.95132052e-05  2.44014463e-05  2.12160671e-05 -5.23635899e-06\n",
      " -1.84788402e-05 -9.47761762e-07  6.72233803e-06  5.92170225e+01\n",
      " -8.72839489e-06 -6.81602373e-06 -5.59098910e-06  9.22865896e+01\n",
      "  2.34463654e-05 -2.59570690e-05  1.63509586e-06 -1.97562776e-05\n",
      "  2.36706051e-05 -2.12230950e-05 -3.84037398e-06  5.16850476e-07\n",
      "  7.41512499e+01  6.19534396e+00 -6.71984550e-06  1.84208151e-06\n",
      "  1.97558578e-05  3.72840887e-05 -7.06500110e-06  8.34218554e-06\n",
      "  1.43288430e-05 -4.96375366e-06  4.08028264e-05 -1.49317782e-05\n",
      " -4.17698406e-06 -2.02320172e-05 -3.41147058e-06 -1.58049915e-05\n",
      " -5.01657156e-06  3.96720540e-05  2.89670270e-06  4.19525858e+00\n",
      "  3.26091424e-05  1.41747909e-05  1.72842797e-05  3.67558208e-06\n",
      " -2.51164161e-07  3.75725496e-05  7.57629330e-06  7.35315779e-06\n",
      "  1.06007274e-05 -2.26595473e-05  1.65137505e-06  1.74966676e-05\n",
      "  1.75473481e-05 -2.12025363e-05 -1.19800421e-05  1.10996744e-05\n",
      "  8.57715489e+01  1.25988658e-07  2.65170683e-06  5.00547153e-05\n",
      " -4.31096494e-05  3.02312545e-05 -9.11204357e-06 -4.15220441e-06\n",
      "  2.40846421e-05 -1.66096961e-05  5.51289878e+01  6.18458420e-06\n",
      "  6.12632003e+01 -7.81993008e-06 -2.88566053e-05  6.43209872e-06\n",
      " -1.22350429e-05  8.20469942e-06  8.66325123e-06  1.32041566e-05\n",
      " -7.17361793e-06 -1.17250435e-05 -2.80263165e-05  2.79674880e-05\n",
      "  2.15105189e-05  4.86011640e-05  1.17338732e-05  6.38947865e+01\n",
      "  1.45230860e-06  5.48620951e-06 -6.48675194e-06 -1.73048055e-05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1).fit(X, y)\n",
    "print('MSE:', mean_squared_error(y, ridge.predict(X)))\n",
    "print('Weights:', ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.755612052048586e-06\n",
      "Weights: [-1.87872977e-05 -1.22371185e-05 -2.40878115e-05 -9.49741419e-06\n",
      " -2.01935811e-05  3.79874415e+01  4.56636036e-06 -2.56372960e-05\n",
      "  4.13593268e-06  1.68358596e-05  2.75271914e-06  1.29567935e-05\n",
      " -1.95273871e-05  2.43789105e-05  2.12466650e-05 -5.19909605e-06\n",
      " -1.84203984e-05 -8.28876418e-07  6.65427618e-06  5.92170224e+01\n",
      " -8.57304558e-06 -6.80485031e-06 -5.45764453e-06  9.22865896e+01\n",
      "  2.34957081e-05 -2.58957105e-05  1.57445020e-06 -1.98544269e-05\n",
      "  2.36495058e-05 -2.13087615e-05 -3.94174972e-06  4.12538609e-07\n",
      "  7.41512498e+01  6.19534392e+00 -6.73367142e-06  1.82692763e-06\n",
      "  1.98666494e-05  3.72595637e-05 -7.12106741e-06  8.40951913e-06\n",
      "  1.43421395e-05 -4.88998276e-06  4.07876129e-05 -1.49774841e-05\n",
      " -4.13150565e-06 -2.02633366e-05 -3.37113456e-06 -1.57852384e-05\n",
      " -4.93841266e-06  3.97318469e-05  3.03082597e-06  4.19525873e+00\n",
      "  3.27230381e-05  1.41719367e-05  1.72758086e-05  3.64947440e-06\n",
      " -2.42234137e-07  3.74674763e-05  7.39631164e-06  7.31560790e-06\n",
      "  1.05942245e-05 -2.26500136e-05  1.73674270e-06  1.75326016e-05\n",
      "  1.75100815e-05 -2.11144546e-05 -1.20639010e-05  1.10777781e-05\n",
      "  8.57715490e+01 -2.62109550e-09  2.72182323e-06  5.00094660e-05\n",
      " -4.30438539e-05  3.02747264e-05 -9.18746135e-06 -4.12630478e-06\n",
      "  2.41125467e-05 -1.66068846e-05  5.51289878e+01  6.27930824e-06\n",
      "  6.12632004e+01 -7.85138714e-06 -2.88045893e-05  6.51816885e-06\n",
      " -1.21914064e-05  8.21214927e-06  8.75769805e-06  1.33121859e-05\n",
      " -7.11541033e-06 -1.18527863e-05 -2.79976143e-05  2.79649821e-05\n",
      "  2.14886490e-05  4.85284188e-05  1.18072041e-05  6.38947865e+01\n",
      "  1.37480946e-06  5.56375910e-06 -6.55496467e-06 -1.74453178e-05]\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression_l2()\n",
    "print('MSE:', mean_squared_error(y, my_reg.predict(X)))\n",
    "print('Weights:', my_reg.fit(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились почти что одинаковые MSE, значит все ок!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
