{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №2: линейная регрессия (10 баллов).\n",
    "\n",
    "Некоторые задания будут по вариантам (всего 4 варианта). Чтобы выяснить свой вариант, посчитайте количество букв в своей фамилии, возьмете остаток от деления на 4 и прибавьте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Считаю свой вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print((len(\"Kazakbaev\") % 4) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Многомерная линейная регрессия из sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим многомерную регрессию из sklearn для стандартного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 10000)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 10000 объектов и 100 признаков. Для начала решим задачу аналитически \"из коробки\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.124696681990535e-25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "reg = LinearRegression().fit(X, y)\n",
    "mse_lin = mean_squared_error(y, reg.predict(X))\n",
    "print(mse_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.124696681990535e-25"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y-reg.predict(X))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обучить линейную регрессию методом градиентного спуска \"из коробки\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.338983326802504e-12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-9.88707175e-09,  2.23074159e-08,  5.34775968e-08, -2.55072391e-09,\n",
       "        6.82681003e-09, -1.32802674e-08, -1.32966179e-08, -8.57583660e-09,\n",
       "        4.61120859e-08, -2.32182314e-08,  3.70500401e+01, -2.64778235e-08,\n",
       "       -1.22552274e-08,  7.48625116e-09,  4.36060209e-08, -3.97312495e-08,\n",
       "       -7.62927819e-08, -4.01389487e-08,  1.39682803e+01,  2.30230716e+01,\n",
       "        1.87018531e-08, -1.17501384e-08, -1.02049391e-08,  2.11091429e-08,\n",
       "       -2.22806107e-09,  1.61346019e-08,  6.31415153e-09,  3.52243407e-09,\n",
       "        7.73774363e-09,  2.36654128e-08, -3.82708042e-08, -6.89005672e-09,\n",
       "       -1.26548462e-08, -4.39147308e-08, -1.18917891e-08, -3.09392044e-08,\n",
       "        2.89222127e-08,  1.43208119e-08,  2.42285928e-08, -1.03792384e-08,\n",
       "        2.59962993e+01,  1.25040507e-08,  4.64551127e+01, -2.87109812e-08,\n",
       "        9.70673003e+01,  3.02971232e-08, -6.03194484e-09,  4.96982251e-08,\n",
       "        3.10850210e-08, -3.40031774e-08, -3.76208805e-08,  3.58790792e-08,\n",
       "       -5.60861233e-08,  2.62307889e+01, -1.58525244e-08,  4.30155348e-09,\n",
       "       -1.45672603e-08,  1.69699011e-08, -5.70972331e-08,  7.97544381e-09,\n",
       "       -1.71599922e-08,  2.26189246e-08, -1.13373975e-08, -4.06177957e-10,\n",
       "        4.97164668e-08, -1.07349801e-09, -4.22957581e-09,  2.91449872e-08,\n",
       "       -1.17559284e-08,  3.20776046e-09, -1.87320149e-08, -3.87618012e-09,\n",
       "        6.20859143e-08,  1.18715269e-08,  1.59549749e-08, -2.51770301e-08,\n",
       "        6.72641977e+01, -4.64123510e-08,  3.84793973e-08, -2.65639233e-08,\n",
       "        7.21078297e+00, -3.76675408e-09,  7.28261004e-08,  1.76609359e-08,\n",
       "        3.20810738e-09,  5.77369921e+01,  2.00131125e-08, -6.08224102e-09,\n",
       "        2.68409572e-08, -3.94128962e-08,  1.16258570e-08, -2.06798092e-08,\n",
       "       -2.37401886e-08, -3.71097283e-08,  5.98030339e-09,  8.50565558e-09,\n",
       "       -2.81024449e-08,  9.36263341e-08, -4.77310016e-09,  1.76010633e-08])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 1 (1 балл).*** Объясните, чем вызвана разница в значениях двух полученных значений метрики?\n",
    "\n",
    "***Задание 2 (1 балл).*** Подберите гиперпараметры в методе градиентного спуска так, чтобы значение MSE было близко к значению MSE, полученному при обучении LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратимся, к документации. Просто Linear Regression минимизирует квадрат разницы того, что неблюдается и то, что предсказано. В то же время, как SGDRegressor сдвигается в сторону антиградиента с определенным шагом - learning rate, и с наличием penalty. У SGDRegressor есть penalty который по дефолту стоит L2, у Linear regression этого нет. \n",
    "\n",
    "Коэффицент $\\alpha$ это множитель регулирязатора(penalty), то есть то насколько сильно мы штрафуем за слишком большие веса. Уменьшая/увеличивая $\\alpha$ будем получать разные MSE.\n",
    "\n",
    "Соответсвенно будет и другой минимизируемая функция: $Q = \\frac {1} {l} \\sum ((w, x_i) - y_i) ^ 2 + \\alpha * \\sum (w_i) ^2$\n",
    "\n",
    "И соотвественно разные MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2764718923605236e-12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "reg = SGDRegressor(alpha=0.00000001).fit(X, y)\n",
    "print(mean_squared_error(y, reg.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это наиболее близкое, что я подбором смог сделать. Сокращая альфу, мы сокращаем MSE. В любом случае получившиеся значение это очень маленькое число."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ваша многомерная линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 3 (5 баллов)***. Напишите собственную многомерную линейную регрессию, оптимизирующую MSE методом *градиентного спуска*. Для этого используйте шаблонный класс. \n",
    "\n",
    "Критерий останова: либо норма разности весов на текущей и предыдущей итерациях меньше определенного значения (первый и третий варианты), либо модуль разности функционалов качества (MSE) на текущей и предыдущей итерациях меньше определенного значения (второй и четвертый варианты). Также предлагается завершать обучение в любом случае, если было произведено слишком много итераций.\n",
    "\n",
    "***Задание 4 (2 балла)***. Добавьте l1 (первый и четвертый варианты) или l2 (второй и третий варианты) регуляризацию. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "class LinearRegression(object):\n",
    "    def __init__(self, learning_rate=0.01, l_ratio=0.001, tol=0.001, epochs=1500, weights=np.zeros(X.shape[1]),\n",
    "        costs = [0]):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        learning_rate - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.epochs = epochs  \n",
    "        self.costs = costs\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y_true = y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        weights = self.weights\n",
    "        costs = self.costs\n",
    "        learning_rate = self.learning_rate\n",
    "        N = float(len(y_true))\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = np.dot(X, weights)\n",
    "            cost = 1 / N * np.sum((y_true - y_pred)** 2)\n",
    "            weights_gradient = (-2 / N) * np.dot(X.T, (y_true - y_pred)) \n",
    "            weights = weights - learning_rate * weights_gradient\n",
    "            costs.append(cost)\n",
    "            if abs(costs[-1] - costs[-2]) < epsilon:\n",
    "                break\n",
    "        return weights\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        y_pr = np.dot(X, self.fit(X, y))\n",
    "        return y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work!\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "epsilon = sys.float_info.epsilon\n",
    "class LinearRegression_l2(object):\n",
    "    def __init__(self, learning_rate=0.01, l_ratio=0.1, tol=0.001, epochs=10000, weights=np.zeros(X.shape[1]),\n",
    "        costs = [0]):\n",
    "        '''\n",
    "        Для начала необходимо инициализировать параметры\n",
    "        learning_rate - это learning rate или шаг обучения\n",
    "        l_ratio - параметр регуляризации\n",
    "        tol - значение для критерия останова\n",
    "        max_iter - максимальное количество итераций обучения\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l_ratio = l_ratio\n",
    "        self.tol = tol\n",
    "        self.epochs = epochs  \n",
    "        self.costs = costs\n",
    "        self.weights = weights\n",
    "    def fit(self, X, y_true = y):\n",
    "        '''\n",
    "        Метод для обучения линейной регрессии\n",
    "        X - матрица признаков\n",
    "        y - вектор правильных ответов\n",
    "        '''\n",
    "        weights = self.weights\n",
    "        costs = self.costs\n",
    "        learning_rate = self.learning_rate\n",
    "        l_ratio = self.l_ratio\n",
    "        N = float(len(y_true))\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = np.dot(X, weights)\n",
    "            cost = 1 / N * np.sum((y_true - y_pred)** 2) + l_ratio * np.sum(weights ** 2)\n",
    "            weights_gradient = (-2 / N) * np.dot(X.T, (y_true - y_pred)) + (l_ratio * 2 * weights) / N\n",
    "            weights = weights - learning_rate * weights_gradient\n",
    "            costs.append(cost)\n",
    "            if abs(costs[-1] - costs[-2]) < epsilon:\n",
    "                break\n",
    "        return weights\n",
    "   \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Метод для предсказаний линейной регрессии\n",
    "        X - матрица признаков\n",
    "        '''\n",
    "        y_pr = np.dot(X, self.fit(X, y))\n",
    "        return y_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are amazing! Great work! even with l2 ratio\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression_l2()\n",
    "my_reg.fit(X, y)\n",
    "assert mean_squared_error(y, my_reg.predict(X)) < 1e-3\n",
    "print('You are amazing! Great work! even with l2 ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Задание 5 (1 балл)***. Обучите линейную регрессию из коробки с l1-регуляризацией (from sklearn.linear_model import Lasso, первый и четвертый варианты) или с l2-регуляризацией (from sklearn.linear_model import Ridge, второй и третий варианты) с значением параметра регуляризации 0.1. Обучите вашу линейную регрессию с тем же значением параметра регуляризации и сравните результаты. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.304540549080835e-06\n",
      "Weights: [ 1.44912814e-06 -1.40678817e-05  9.70612687e-06  5.58687665e-06\n",
      " -1.14165171e-05  5.92121595e-06 -5.68121728e-06 -2.25606353e-05\n",
      "  7.64011158e-06 -4.40383365e-06  3.70496800e+01 -9.32097248e-06\n",
      " -5.45192966e-06 -3.86092962e-06  3.35516111e-06 -1.73841099e-05\n",
      " -4.87845232e-06 -1.44614508e-05  1.39681483e+01  2.30228151e+01\n",
      "  2.13578079e-05 -1.29382940e-05  3.07403323e-05 -8.94947936e-06\n",
      "  4.12011243e-06  1.13374918e-05 -3.27813478e-06 -4.83473953e-06\n",
      "  1.63519856e-05  9.99138198e-07  8.97551827e-06 -1.85152318e-05\n",
      " -3.13605184e-05 -1.51604486e-05 -1.68707470e-05 -7.00610174e-06\n",
      "  2.33408795e-06  9.73752982e-06  7.17135576e-06 -8.46942820e-06\n",
      "  2.59960079e+01 -6.96844706e-06  4.64546467e+01 -2.78731343e-05\n",
      "  9.70663623e+01 -4.06741221e-06 -9.49344741e-06 -8.06269277e-07\n",
      "  6.68509241e-06 -4.19933593e-05 -3.15957390e-05  3.15899221e-06\n",
      " -1.75395754e-05  2.62305096e+01 -6.68419809e-06 -1.69394344e-06\n",
      "  1.38114745e-05  3.67280909e-06 -1.24008620e-05  6.04617084e-07\n",
      "  1.01128558e-05  2.06635047e-06  1.56538917e-06  4.39195952e-06\n",
      "  2.90197933e-06 -1.98828972e-06  1.12079643e-05  4.59840475e-06\n",
      "  1.63532977e-05  7.16306680e-06 -1.43017497e-05  7.96372276e-06\n",
      "  1.04778145e-05  1.81715452e-05  7.37632180e-07 -2.49525927e-05\n",
      "  6.72634984e+01 -9.14918369e-06  8.67207288e-06 -3.23506270e-05\n",
      "  7.21073012e+00 -1.13873951e-05  2.36284253e-05  1.92774002e-06\n",
      " -8.85191919e-07  5.77364012e+01 -2.08454883e-07  4.41943840e-06\n",
      "  3.55610954e-05 -1.66586811e-05 -8.11500185e-06 -1.39144930e-06\n",
      "  7.67784181e-07  4.01160163e-06 -1.22688706e-05  1.40578794e-05\n",
      " -3.82432035e-06  4.76264255e-05  9.34056860e-06 -3.87713121e-06]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1).fit(X, y)\n",
    "print('MSE:', mean_squared_error(y, ridge.predict(X)))\n",
    "print('Weights:', ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2.3045106007388436e-06\n",
      "Weights: [ 1.44391714e-06 -1.40811132e-05  9.69690207e-06  5.63175961e-06\n",
      " -1.14262941e-05  6.00734738e-06 -5.64246212e-06 -2.26048990e-05\n",
      "  7.70920670e-06 -4.37313713e-06  3.70496800e+01 -9.35245353e-06\n",
      " -5.40895020e-06 -3.82505803e-06  3.34849577e-06 -1.74279227e-05\n",
      " -4.83496481e-06 -1.45468800e-05  1.39681484e+01  2.30228151e+01\n",
      "  2.13324623e-05 -1.29961190e-05  3.07695858e-05 -8.82124015e-06\n",
      "  4.06549066e-06  1.13039332e-05 -3.28987091e-06 -4.74460016e-06\n",
      "  1.63346379e-05  9.88384161e-07  9.05636925e-06 -1.84559008e-05\n",
      " -3.12814868e-05 -1.52415704e-05 -1.69805540e-05 -7.03282980e-06\n",
      "  2.24349777e-06  9.68238861e-06  7.12345143e-06 -8.56979132e-06\n",
      "  2.59960079e+01 -6.94842869e-06  4.64546468e+01 -2.78993954e-05\n",
      "  9.70663623e+01 -4.06292261e-06 -9.43637996e-06 -6.74156627e-07\n",
      "  6.75907137e-06 -4.18911525e-05 -3.16243252e-05  3.26055514e-06\n",
      " -1.74827320e-05  2.62305096e+01 -6.66441651e-06 -1.71439132e-06\n",
      "  1.38025562e-05  3.69488841e-06 -1.23023368e-05  6.45882588e-07\n",
      "  1.00527487e-05  2.10457220e-06  1.59386408e-06  4.34704060e-06\n",
      "  2.94384381e-06 -1.93069874e-06  1.11242536e-05  4.63910738e-06\n",
      "  1.63917586e-05  7.15237221e-06 -1.42966855e-05  7.95525966e-06\n",
      "  1.04152315e-05  1.82126203e-05  7.18138765e-07 -2.49026181e-05\n",
      "  6.72634985e+01 -9.11073757e-06  8.63237126e-06 -3.23729542e-05\n",
      "  7.21073007e+00 -1.12374316e-05  2.35477204e-05  1.89041650e-06\n",
      " -9.27320436e-07  5.77364012e+01 -1.70460255e-07  4.41267821e-06\n",
      "  3.56312510e-05 -1.66813589e-05 -8.23529971e-06 -1.39022744e-06\n",
      "  7.75764279e-07  3.98870551e-06 -1.22162817e-05  1.40802783e-05\n",
      " -3.86730821e-06  4.75805377e-05  9.40428783e-06 -4.01536081e-06]\n"
     ]
    }
   ],
   "source": [
    "my_reg = LinearRegression_l2()\n",
    "print('MSE:', mean_squared_error(y, my_reg.predict(X)))\n",
    "print('Weights:', my_reg.fit(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получились почти что одинаковые MSE, значит все ок!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
