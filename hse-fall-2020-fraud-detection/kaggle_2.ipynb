{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "Merging data\n",
      "(417559, 436) (172981, 435)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/train2.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b521434b3d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test2.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(self, path, compression, protocol)\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2725\u001b[0;31m         \u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2727\u001b[0m     def to_clipboard(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mto_pickle\u001b[0;34m(obj, filepath_or_buffer, compression, protocol)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"infer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train2.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# ------------------ Inputs ----------------------\n",
    "# ------------------------------------------------\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "LOCAL_TEST = True\n",
    "folder_path = r'../input'\n",
    "project_path = r'...'\n",
    "\n",
    "# ------------------------------------------------\n",
    "\n",
    "print('Load Data')\n",
    "\n",
    "train_df = pd.read_csv('train_transaction.csv')\n",
    "test_df = pd.read_csv('test_transaction.csv')\n",
    "\n",
    "train_identity = pd.read_csv('train_identity.csv')\n",
    "test_identity = pd.read_csv('test_identity.csv')\n",
    "\n",
    "\n",
    "def sd(col, max_loss_limit=0.001, avg_loss_limit=0.001, na_loss_limit=0, n_uniq_loss_limit=0, fillna=0):\n",
    "    \"\"\"\n",
    "    max_loss_limit - don't allow any float to lose precision more than this value. Any values are ok for GBT algorithms as long as you don't unique values.\n",
    "                     See https://en.wikipedia.org/wiki/Half-precision_floating-point_format#Precision_limitations_on_decimal_values_in_[0,_1]\n",
    "    avg_loss_limit - same but calculates avg throughout the series.\n",
    "    na_loss_limit - not really useful.\n",
    "    n_uniq_loss_limit - very important parameter. If you have a float field with very high cardinality you can set this value to something like n_records * 0.01 in order to allow some field relaxing.\n",
    "    \"\"\"\n",
    "    is_float = str(col.dtypes)[:5] == 'float'\n",
    "    na_count = col.isna().sum()\n",
    "    n_uniq = col.nunique(dropna=False)\n",
    "    try_types = ['float16', 'float32']\n",
    "\n",
    "    if na_count <= na_loss_limit:\n",
    "        try_types = ['int8', 'int16', 'float16', 'int32', 'float32']\n",
    "\n",
    "    for type in try_types:\n",
    "        col_tmp = col\n",
    "\n",
    "        # float to int conversion => try to round to minimize casting error\n",
    "        if is_float and (str(type)[:3] == 'int'):\n",
    "            col_tmp = col_tmp.copy().fillna(fillna).round()\n",
    "\n",
    "        col_tmp = col_tmp.astype(type)\n",
    "        max_loss = (col_tmp - col).abs().max()\n",
    "        avg_loss = (col_tmp - col).abs().mean()\n",
    "        na_loss = np.abs(na_count - col_tmp.isna().sum())\n",
    "        n_uniq_loss = np.abs(n_uniq - col_tmp.nunique(dropna=False))\n",
    "\n",
    "        if max_loss <= max_loss_limit and avg_loss <= avg_loss_limit and na_loss <= na_loss_limit and n_uniq_loss <= n_uniq_loss_limit:\n",
    "            return col_tmp\n",
    "\n",
    "    # field can't be converted\n",
    "    return col\n",
    "\n",
    "\n",
    "def reduce_mem_usage_sd(df, deep=True, verbose=False, obj_to_cat=False):\n",
    "    numerics = ['int16', 'uint16', 'int32', 'uint32', 'int64', 'uint64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage(deep=deep).sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "\n",
    "        # collect stats\n",
    "        na_count = df[col].isna().sum()\n",
    "        n_uniq = df[col].nunique(dropna=False)\n",
    "\n",
    "        # numerics\n",
    "        if col_type in numerics:\n",
    "            df[col] = sd(df[col])\n",
    "\n",
    "        # strings\n",
    "        if (col_type == 'object') and obj_to_cat:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Column {col}: {col_type} -> {df[col].dtypes}, na_count={na_count}, n_uniq={n_uniq}')\n",
    "        new_na_count = df[col].isna().sum()\n",
    "        if (na_count != new_na_count):\n",
    "            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost na values. Before: {na_count}, after: {new_na_count}')\n",
    "        new_n_uniq = df[col].nunique(dropna=False)\n",
    "        if (n_uniq != new_n_uniq):\n",
    "            print(f'Warning: column {col}, {col_type} -> {df[col].dtypes} lost unique values. Before: {n_uniq}, after: {new_n_uniq}')\n",
    "\n",
    "    end_mem = df.memory_usage(deep=deep).sum() / 1024 ** 2\n",
    "    percent = 100 * (start_mem - end_mem) / start_mem\n",
    "    if verbose:\n",
    "        print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem,\n",
    "                                                                                              percent))\n",
    "    return df\n",
    "\n",
    "\n",
    "def minify_identity_df(df):\n",
    "    df['id_12'] = df['id_12'].map({'Found': 1, 'NotFound': 0})\n",
    "    df['id_15'] = df['id_15'].map({'New': 2, 'Found': 1, 'Unknown': 0})\n",
    "    df['id_16'] = df['id_16'].map({'Found': 1, 'NotFound': 0})\n",
    "\n",
    "    df['id_23'] = df['id_23'].map({'TRANSPARENT': 4, 'IP_PROXY': 3, 'IP_PROXY:ANONYMOUS': 2, 'IP_PROXY:HIDDEN': 1})\n",
    "\n",
    "    df['id_27'] = df['id_27'].map({'Found': 1, 'NotFound': 0})\n",
    "    df['id_28'] = df['id_28'].map({'New': 2, 'Found': 1})\n",
    "\n",
    "    df['id_29'] = df['id_29'].map({'Found': 1, 'NotFound': 0})\n",
    "\n",
    "    df['id_35'] = df['id_35'].map({'T': 1, 'F': 0})\n",
    "    df['id_36'] = df['id_36'].map({'T': 1, 'F': 0})\n",
    "    df['id_37'] = df['id_37'].map({'T': 1, 'F': 0})\n",
    "    df['id_38'] = df['id_38'].map({'T': 1, 'F': 0})\n",
    "\n",
    "    df['id_34'] = df['id_34'].fillna(':0')\n",
    "    df['id_34'] = df['id_34'].apply(lambda x: x.split(':')[1]).astype(np.int8)\n",
    "    df['id_34'] = np.where(df['id_34'] == 0, np.nan, df['id_34'])\n",
    "\n",
    "    df['id_33'] = df['id_33'].fillna('0x0')\n",
    "    df['id_33_0'] = df['id_33'].apply(lambda x: x.split('x')[0]).astype(int)\n",
    "    df['id_33_1'] = df['id_33'].apply(lambda x: x.split('x')[1]).astype(int)\n",
    "    df['id_33'] = np.where(df['id_33'] == '0x0', np.nan, df['id_33'])\n",
    "\n",
    "    df['DeviceType'].map({'desktop': 1, 'mobile': 0})\n",
    "    return df\n",
    "\n",
    "\n",
    "train_identity = minify_identity_df(train_identity)\n",
    "test_identity = minify_identity_df(test_identity)\n",
    "\n",
    "train_identity['id_33'] = train_identity['id_33'].fillna('unseen_before_label')\n",
    "test_identity['id_33'] = test_identity['id_33'].fillna('unseen_before_label')\n",
    "\n",
    "train_df = reduce_mem_usage_sd(train_df)\n",
    "test_df = reduce_mem_usage_sd(test_df)\n",
    "\n",
    "train_identity = reduce_mem_usage_sd(train_identity)\n",
    "test_identity = reduce_mem_usage_sd(test_identity)\n",
    "\n",
    "\n",
    "print('Merging data')\n",
    "train = pd.merge(train_df, train_identity, on='TransactionID', how='left')\n",
    "test = pd.merge(test_df, test_identity, on='TransactionID', how='left')\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "train.to_pickle(os.path.join(folder_path, 'train2.pkl'))\n",
    "test.to_pickle(os.path.join(folder_path, 'test2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
