{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Предобработка данных. Категориальные признаки. Работа с текстами.**\n",
    "\n",
    "Содержание семинара опирается на семинары, проводимые на ФКН. Во второй части работы отчасти использован туториал Kaggle по Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первой части семинара будем использовать немного видоизменённый для наших целей датасет https://archive.ics.uci.edu/ml/datasets/AutoUniv.\n",
    "\n",
    "В нём присутствуют целочисленные, вещественнозначные и категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>att10</th>\n",
       "      <th>...</th>\n",
       "      <th>att117</th>\n",
       "      <th>att118</th>\n",
       "      <th>att119</th>\n",
       "      <th>att120</th>\n",
       "      <th>att121</th>\n",
       "      <th>att122</th>\n",
       "      <th>att123</th>\n",
       "      <th>att124</th>\n",
       "      <th>att125</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>550</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>78.2</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.53</td>\n",
       "      <td>83.7</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4753</td>\n",
       "      <td>904</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.9</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.6</td>\n",
       "      <td>294</td>\n",
       "      <td>739</td>\n",
       "      <td>class4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.1</td>\n",
       "      <td>542</td>\n",
       "      <td>70.2</td>\n",
       "      <td>4.04</td>\n",
       "      <td>79.1</td>\n",
       "      <td>6464</td>\n",
       "      <td>1.56</td>\n",
       "      <td>94.7</td>\n",
       "      <td>seal</td>\n",
       "      <td>0.738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6952</td>\n",
       "      <td>905</td>\n",
       "      <td>0.91</td>\n",
       "      <td>5.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>660</td>\n",
       "      <td>705</td>\n",
       "      <td>class4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>579</td>\n",
       "      <td>90.9</td>\n",
       "      <td>4.16</td>\n",
       "      <td>76.7</td>\n",
       "      <td>6395</td>\n",
       "      <td>7.27</td>\n",
       "      <td>84.4</td>\n",
       "      <td>seal</td>\n",
       "      <td>0.737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4664</td>\n",
       "      <td>897</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.1</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.7</td>\n",
       "      <td>546</td>\n",
       "      <td>688</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>609</td>\n",
       "      <td>78.2</td>\n",
       "      <td>3.02</td>\n",
       "      <td>78.6</td>\n",
       "      <td>5678</td>\n",
       "      <td>8.04</td>\n",
       "      <td>88.2</td>\n",
       "      <td>bird</td>\n",
       "      <td>0.806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4762</td>\n",
       "      <td>873</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>v2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>646</td>\n",
       "      <td>783</td>\n",
       "      <td>class4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.7</td>\n",
       "      <td>557</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>78.9</td>\n",
       "      <td>6508</td>\n",
       "      <td>9.00</td>\n",
       "      <td>84.5</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48</td>\n",
       "      <td>4649</td>\n",
       "      <td>822</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>642</td>\n",
       "      <td>665</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   att1  att2  att3  att4  att5  att6  att7  att8  att9  att10  ...  att117  \\\n",
       "0   6.4   550  92.0  3.15  78.2  6040  1.53  83.7   dog  0.791  ...    0.77   \n",
       "1   3.1   542  70.2  4.04  79.1  6464  1.56  94.7  seal  0.738  ...    0.45   \n",
       "2   3.2   579  90.9  4.16  76.7  6395  7.27  84.4  seal  0.737  ...    0.45   \n",
       "3   3.1   609  78.2  3.02  78.6  5678  8.04  88.2  bird  0.806  ...    0.56   \n",
       "4   2.7   557  81.0  4.03  78.9  6508  9.00  84.5   cat  0.790  ...    0.48   \n",
       "\n",
       "   att118  att119 att120  att121  att122 att123  att124  att125   class  \n",
       "0    4753     904   0.93     2.9      v1   27.6     294     739  class4  \n",
       "1    6952     905   0.91     5.2      v1   28.5     660     705  class4  \n",
       "2    4664     897   0.84     1.1      v1   27.7     546     688  class1  \n",
       "3    4762     873   0.48     1.3      v2   27.7     646     783  class4  \n",
       "4    4649     822   0.86     1.2      v1   47.0     642     665  class1  \n",
       "\n",
       "[5 rows x 126 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('table.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>att10</th>\n",
       "      <th>...</th>\n",
       "      <th>att116</th>\n",
       "      <th>att117</th>\n",
       "      <th>att118</th>\n",
       "      <th>att119</th>\n",
       "      <th>att120</th>\n",
       "      <th>att121</th>\n",
       "      <th>att122</th>\n",
       "      <th>att123</th>\n",
       "      <th>att124</th>\n",
       "      <th>att125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>550</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>78.2</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.53</td>\n",
       "      <td>83.7</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.791</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4753</td>\n",
       "      <td>904</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.9</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.6</td>\n",
       "      <td>294</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.1</td>\n",
       "      <td>542</td>\n",
       "      <td>70.2</td>\n",
       "      <td>4.04</td>\n",
       "      <td>79.1</td>\n",
       "      <td>6464</td>\n",
       "      <td>1.56</td>\n",
       "      <td>94.7</td>\n",
       "      <td>seal</td>\n",
       "      <td>0.738</td>\n",
       "      <td>...</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6952</td>\n",
       "      <td>905</td>\n",
       "      <td>0.91</td>\n",
       "      <td>5.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>660</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>579</td>\n",
       "      <td>90.9</td>\n",
       "      <td>4.16</td>\n",
       "      <td>76.7</td>\n",
       "      <td>6395</td>\n",
       "      <td>7.27</td>\n",
       "      <td>84.4</td>\n",
       "      <td>seal</td>\n",
       "      <td>0.737</td>\n",
       "      <td>...</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4664</td>\n",
       "      <td>897</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.1</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.7</td>\n",
       "      <td>546</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>609</td>\n",
       "      <td>78.2</td>\n",
       "      <td>3.02</td>\n",
       "      <td>78.6</td>\n",
       "      <td>5678</td>\n",
       "      <td>8.04</td>\n",
       "      <td>88.2</td>\n",
       "      <td>bird</td>\n",
       "      <td>0.806</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4762</td>\n",
       "      <td>873</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>v2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>646</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.7</td>\n",
       "      <td>557</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>78.9</td>\n",
       "      <td>6508</td>\n",
       "      <td>9.00</td>\n",
       "      <td>84.5</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.790</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.48</td>\n",
       "      <td>4649</td>\n",
       "      <td>822</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>642</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   att1  att2  att3  att4  att5  att6  att7  att8  att9  att10  ...  att116  \\\n",
       "0   6.4   550  92.0  3.15  78.2  6040  1.53  83.7   dog  0.791  ...      v3   \n",
       "1   3.1   542  70.2  4.04  79.1  6464  1.56  94.7  seal  0.738  ...      v2   \n",
       "2   3.2   579  90.9  4.16  76.7  6395  7.27  84.4  seal  0.737  ...      v1   \n",
       "3   3.1   609  78.2  3.02  78.6  5678  8.04  88.2  bird  0.806  ...      v3   \n",
       "4   2.7   557  81.0  4.03  78.9  6508  9.00  84.5   cat  0.790  ...      v3   \n",
       "\n",
       "   att117  att118 att119  att120  att121 att122  att123  att124  att125  \n",
       "0    0.77    4753    904    0.93     2.9     v1    27.6     294     739  \n",
       "1    0.45    6952    905    0.91     5.2     v1    28.5     660     705  \n",
       "2    0.45    4664    897    0.84     1.1     v1    27.7     546     688  \n",
       "3    0.56    4762    873    0.48     1.3     v2    27.7     646     783  \n",
       "4    0.48    4649    822    0.86     1.2     v1    47.0     642     665  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['class']\n",
    "X = df.drop(['class'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 2, 0, 2, 0, 3, 0, 3,\n",
       "       3, 3, 0, 3, 2, 3, 0, 3, 0, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "       3, 3, 2, 3, 0, 0, 0, 2, 0, 2, 0, 1, 0, 2, 3, 1, 0, 0, 0, 3, 0, 0,\n",
       "       0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 3, 2, 2, 0, 0, 0, 1, 3, 0, 0, 0, 0,\n",
       "       0, 3, 0, 0, 3, 3, 0, 2, 0, 0, 1, 0, 3, 0, 0, 2, 3, 0, 3, 0, 0, 3,\n",
       "       0, 2, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0,\n",
       "       3, 3, 3, 3, 0, 0, 2, 0, 0, 3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 2, 3,\n",
       "       0, 0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 2, 3, 0, 0, 0, 3, 3, 0,\n",
       "       1, 0, 3, 3, 2, 3, 3, 0, 2, 3, 2, 2, 0, 3, 0, 1, 0, 0, 0, 3, 0, 0,\n",
       "       2, 0, 2, 0, 0, 0, 2, 3, 0, 3, 0, 0, 3, 1, 0, 1, 3, 2, 0, 2, 0, 0,\n",
       "       3, 0, 3, 0, 3, 2, 0, 0, 3, 3, 3, 3, 0, 1, 3, 3, 3, 0, 0, 0, 0, 0,\n",
       "       0, 3, 3, 0, 2, 3, 2, 3, 3, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 1,\n",
       "       0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 3, 3, 3, 0, 0, 3, 3, 0, 0, 0, 3,\n",
       "       0, 0, 0, 2, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 3, 0,\n",
       "       0, 2, 3, 3, 2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 3, 0,\n",
       "       0, 3, 0, 3, 0, 3, 0, 0, 2, 2, 0, 3, 0, 0, 3, 2, 0, 0, 0, 0, 2, 1,\n",
       "       3, 0, 3, 0, 0, 3, 3, 3, 0, 3, 0, 0, 3, 3, 0, 0, 3, 0, 0, 0, 1, 0,\n",
       "       3, 0, 0, 0, 3, 0, 2, 3, 3, 0, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0,\n",
       "       3, 0, 0, 0, 0, 3, 3, 0, 3, 0, 3, 0, 0, 3, 0, 3, 0, 2, 3, 0, 3, 0,\n",
       "       0, 0, 0, 0, 0, 3, 0, 2, 1, 0, 0, 0, 0, 2, 0, 3, 0, 0, 0, 0, 3, 2,\n",
       "       0, 0, 0, 3, 0, 3, 3, 0, 0, 0, 3, 0, 2, 1, 3, 0, 0, 2, 3, 0, 3, 0,\n",
       "       3, 3, 1, 0, 2, 1, 0, 3, 0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 3, 0, 3, 0,\n",
       "       2, 2, 3, 0, 3, 3, 3, 0, 0, 3, 2, 0, 3, 0, 0, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc = preprocessing.LabelEncoder()\n",
    "y = label_enc.fit_transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Преобразование категориальных признаков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько типов предобработки данных. Она важна для корректной работы различных методов машинного обучения, в особенности, для линейных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в наших данных присутствуют категориальные признаки, то есть текстовые или порядковые признаки - те признаки, к которым нельзя относиться как к числам, то для дальнешей работы с такими данными надо преобразовать категориальные признаки в числа. \n",
    "\n",
    "Нельзя упускать из вида порядковые признаки. Например, если наши данные содержат в качестве столбца индекс местности, то алгоритм будет считать, что индекс 119331 > 119101, что для нас смысла не имеет. Мы хотим, чтобы различные индексы служили индикаторами различных географических областей. Для этого порядковые признаки также надо предобрабатывать и переводить в числовые."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие стоблцы нашей таблицы содержат категориальные признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True, False, False,  True, False,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False,  True,  True, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False,  True,  True, False, False, False,  True, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False,  True, False,\n",
       "       False, False, False,  True, False, False, False,  True, False,\n",
       "        True, False, False, False, False,  True, False, False, False,\n",
       "       False,  True, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False,  True, False, False, False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features_mask = (X.dtypes == \"object\").values\n",
    "cat_features_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_features_mask[cat_features_mask==True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 способ кодирования: счётчики**\n",
    "\n",
    "Мы можем посчитать, сколько раз каждое значение встречалось в таблице и заменить каждый категориальный признак соответствующим счетчиком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['att9', 'att14', 'att17', 'att24', 'att25', 'att28', 'att35', 'att42',\n",
       "       'att43', 'att48', 'att57', 'att58', 'att62', 'att65', 'att78', 'att80',\n",
       "       'att85', 'att89', 'att91', 'att96', 'att101', 'att104', 'att116',\n",
       "       'att122'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_copy.columns[cat_features_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>att10</th>\n",
       "      <th>...</th>\n",
       "      <th>att116</th>\n",
       "      <th>att117</th>\n",
       "      <th>att118</th>\n",
       "      <th>att119</th>\n",
       "      <th>att120</th>\n",
       "      <th>att121</th>\n",
       "      <th>att122</th>\n",
       "      <th>att123</th>\n",
       "      <th>att124</th>\n",
       "      <th>att125</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>550</td>\n",
       "      <td>92.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>78.2</td>\n",
       "      <td>6040</td>\n",
       "      <td>1.53</td>\n",
       "      <td>83.7</td>\n",
       "      <td>78</td>\n",
       "      <td>0.791</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.77</td>\n",
       "      <td>4753</td>\n",
       "      <td>904</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.9</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.6</td>\n",
       "      <td>294</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.1</td>\n",
       "      <td>542</td>\n",
       "      <td>70.2</td>\n",
       "      <td>4.04</td>\n",
       "      <td>79.1</td>\n",
       "      <td>6464</td>\n",
       "      <td>1.56</td>\n",
       "      <td>94.7</td>\n",
       "      <td>80</td>\n",
       "      <td>0.738</td>\n",
       "      <td>...</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>6952</td>\n",
       "      <td>905</td>\n",
       "      <td>0.91</td>\n",
       "      <td>5.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>660</td>\n",
       "      <td>705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.2</td>\n",
       "      <td>579</td>\n",
       "      <td>90.9</td>\n",
       "      <td>4.16</td>\n",
       "      <td>76.7</td>\n",
       "      <td>6395</td>\n",
       "      <td>7.27</td>\n",
       "      <td>84.4</td>\n",
       "      <td>80</td>\n",
       "      <td>0.737</td>\n",
       "      <td>...</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>4664</td>\n",
       "      <td>897</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.1</td>\n",
       "      <td>v1</td>\n",
       "      <td>27.7</td>\n",
       "      <td>546</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.1</td>\n",
       "      <td>609</td>\n",
       "      <td>78.2</td>\n",
       "      <td>3.02</td>\n",
       "      <td>78.6</td>\n",
       "      <td>5678</td>\n",
       "      <td>8.04</td>\n",
       "      <td>88.2</td>\n",
       "      <td>79</td>\n",
       "      <td>0.806</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.56</td>\n",
       "      <td>4762</td>\n",
       "      <td>873</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>v2</td>\n",
       "      <td>27.7</td>\n",
       "      <td>646</td>\n",
       "      <td>783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.7</td>\n",
       "      <td>557</td>\n",
       "      <td>81.0</td>\n",
       "      <td>4.03</td>\n",
       "      <td>78.9</td>\n",
       "      <td>6508</td>\n",
       "      <td>9.00</td>\n",
       "      <td>84.5</td>\n",
       "      <td>72</td>\n",
       "      <td>0.790</td>\n",
       "      <td>...</td>\n",
       "      <td>v3</td>\n",
       "      <td>0.48</td>\n",
       "      <td>4649</td>\n",
       "      <td>822</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.2</td>\n",
       "      <td>v1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>642</td>\n",
       "      <td>665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   att1  att2  att3  att4  att5  att6  att7  att8  att9  att10  ...  att116  \\\n",
       "0   6.4   550  92.0  3.15  78.2  6040  1.53  83.7    78  0.791  ...      v3   \n",
       "1   3.1   542  70.2  4.04  79.1  6464  1.56  94.7    80  0.738  ...      v2   \n",
       "2   3.2   579  90.9  4.16  76.7  6395  7.27  84.4    80  0.737  ...      v1   \n",
       "3   3.1   609  78.2  3.02  78.6  5678  8.04  88.2    79  0.806  ...      v3   \n",
       "4   2.7   557  81.0  4.03  78.9  6508  9.00  84.5    72  0.790  ...      v3   \n",
       "\n",
       "   att117  att118 att119  att120  att121 att122  att123  att124  att125  \n",
       "0    0.77    4753    904    0.93     2.9     v1    27.6     294     739  \n",
       "1    0.45    6952    905    0.91     5.2     v1    28.5     660     705  \n",
       "2    0.45    4664    897    0.84     1.1     v1    27.7     546     688  \n",
       "3    0.56    4762    873    0.48     1.3     v2    27.7     646     783  \n",
       "4    0.48    4649    822    0.86     1.2     v1    47.0     642     665  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for object in X.columns[cat_features_mask]:\n",
    "    for name, count in X[object].value_counts().items():\n",
    "        X_copy[objects].replace(name, count, inplace = True)\n",
    "X_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 способ кодирования: OneHot-кодирование**\n",
    "\n",
    "Пусть некоторый признак принимает значения из множества K. OneHotEncoder вместо одного признака создает K бинарных признаков - по одному на каждое возможное значение исходного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(sparse=False, drop='first')\n",
    "X_cat = enc.fit_transform(X_copy[X_copy.columns[cat_features_mask]])\n",
    "X_cat = pd.DataFrame(data=X_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 53)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4    5    6    7    8    9   ...   43   44   45   46  \\\n",
       "0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  ...  0.0  0.0  0.0  1.0   \n",
       "1  0.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  ...  0.0  1.0  0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  0.0  1.0  ...  0.0  1.0  1.0  0.0   \n",
       "3  0.0  0.0  0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  ...  0.0  0.0  0.0  0.0   \n",
       "4  0.0  1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  1.0  1.0  0.0  1.0   \n",
       "\n",
       "    47   48   49   50   51   52  \n",
       "0  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "1  0.0  0.0  0.0  1.0  0.0  0.0  \n",
       "2  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  1.0  0.0  0.0  0.0  1.0  1.0  \n",
       "4  0.0  1.0  0.0  0.0  1.0  0.0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_cat.shape)\n",
    "X_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 способ кодирования: хэширование**\n",
    "\n",
    "HashingVectorizer преобразовывает строку в числовой массив заданной длиной с помощью хэш-функции. В этом методе в качестве входных параметров мы задаем желаемое количество новых признаков, а также токенизатор - обработчик текста (в нём мы можем сделать любую удобную нам предобработку текста: удалить редкие слова, удалить знаки препинания, оставить только слова из определенного списка и т.д.). Токенизатор возвращает текст, разбитый на токены, т.е. на слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(s):\n",
    "    return [elem for elem in s.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее интересный для нас с точки зрения хэширования - столбец att9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     78\n",
       "1     80\n",
       "3     79\n",
       "4     72\n",
       "5     55\n",
       "14    63\n",
       "19    73\n",
       "Name: att9, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_copy['att9'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для применения HashingVectorize выбираем из столбца все различные значения(слова) без повторений, обучаем HashingVectorizer на этих словах и применяем ко всему столбцу. В итоге мы получаем разреженную матрицу. С ней умеют работать многие алгоритмы машинного обучения, но при желании можем перевести ее в numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.int64' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9392d72805c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_wo_duple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_copy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'att9'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/_hash.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_X)\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             _hashing_transform(raw_X, self.n_features, self.dtype,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                self.alternate_sign, seed=0)\n\u001b[1;32m    161\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/feature_extraction/_hashing_fast.pyx\u001b[0m in \u001b[0;36msklearn.feature_extraction._hashing_fast.transform\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/_hash.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mraw_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_iteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mraw_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             _hashing_transform(raw_X, self.n_features, self.dtype,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0manalyzer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_hasher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \"\"\"\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.int64' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "coder = HashingVectorizer(tokenizer=my_tokenizer, n_features=3, norm='l2', alternate_sign=True)\n",
    "\n",
    "train_wo_duple = X_copy['att9'].drop_duplicates()\n",
    "coder.fit(train_wo_duple)\n",
    "\n",
    "coder.transform(X_copy['att9'].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Заполнение пропусков**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходных данных могут быть пропущенные значения. Большинство методов машинного обучения не умеют с ними работать. Для этого мы должны каким-нибудь образом заполнить эти пропуски.\n",
    "\n",
    "Способы заполнения пропусков:\n",
    "\n",
    "* средним значением \n",
    "* медианой\n",
    "* самым часто встречающимся значением\n",
    "* каким-то одним новым значением (иногда пропуск в данных можно воспринимать как еще одно категориальное значение).\n",
    "* можно взять часть данных для обучения, а на другой части данных предсказать пропущенные значения (тогда для решения основной задачи нельзя обучаться на первой части данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, есть ли пропуски в наших данных и много ли их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  5,  14,  25,  26,  27,  66,  67,  69,  74,  83,  92,  99, 110,\n",
       "       123, 124, 125, 343, 344, 345, 348, 352, 362])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#индексы строк с NAN\n",
    "\n",
    "X_real = X_copy[X_copy.columns[~cat_features_mask]]\n",
    "\n",
    "print(np.any(np.isnan(X_real)))\n",
    "\n",
    "np.array(pd.isnull(X_real).any(1)).nonzero()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним пропуски в данных с помощью встроенного метода Imputer, используя среднее значение признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "mis_replacer = SimpleImputer(strategy=\"mean\")\n",
    "X_no_mis = pd.DataFrame(data=mis_replacer.fit_transform(X_real))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(X_no_mis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Масштабирование признаков**\n",
    "\n",
    "Для применения в особенности линейных методов машинного обучения масштабирование признаков очень важно. Может быть так, что метод даст совершенно неправильный результат без масштабирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Масштабирование признаков можно выполнить, например, одним из следующих способов:\n",
    " - $x_{new} = \\dfrac{x - \\mu}{\\sigma}$, где $\\mu, \\sigma$ — среднее и стандартное отклонение значения признака по всей выборке (StandardScaler в sklearn)\n",
    " - $x_{new} = \\dfrac{x - x_{min}}{x_{max} - x_{min}}$, где $[x_{min}, x_{max}]$ — минимальный интервал значений признака (MinMaxScaler в sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.900803</td>\n",
       "      <td>-1.008627</td>\n",
       "      <td>1.673073</td>\n",
       "      <td>-0.882219</td>\n",
       "      <td>0.263659</td>\n",
       "      <td>-0.765146</td>\n",
       "      <td>-1.651706</td>\n",
       "      <td>-1.044694</td>\n",
       "      <td>0.332922</td>\n",
       "      <td>1.502317</td>\n",
       "      <td>...</td>\n",
       "      <td>1.259626</td>\n",
       "      <td>-2.069681</td>\n",
       "      <td>1.088027</td>\n",
       "      <td>-1.003417</td>\n",
       "      <td>1.203292</td>\n",
       "      <td>1.221725</td>\n",
       "      <td>-0.379508</td>\n",
       "      <td>-1.199808</td>\n",
       "      <td>-2.073145</td>\n",
       "      <td>0.362206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.458755</td>\n",
       "      <td>-1.327007</td>\n",
       "      <td>-0.996571</td>\n",
       "      <td>0.592122</td>\n",
       "      <td>1.366884</td>\n",
       "      <td>-0.344978</td>\n",
       "      <td>-1.641333</td>\n",
       "      <td>1.354094</td>\n",
       "      <td>-1.004017</td>\n",
       "      <td>0.370001</td>\n",
       "      <td>...</td>\n",
       "      <td>1.202224</td>\n",
       "      <td>-1.932185</td>\n",
       "      <td>-1.639284</td>\n",
       "      <td>0.760586</td>\n",
       "      <td>1.213074</td>\n",
       "      <td>1.131508</td>\n",
       "      <td>0.996822</td>\n",
       "      <td>-1.139352</td>\n",
       "      <td>0.609902</td>\n",
       "      <td>-0.001928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.387254</td>\n",
       "      <td>0.145500</td>\n",
       "      <td>1.538366</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>-1.575049</td>\n",
       "      <td>-0.413355</td>\n",
       "      <td>0.332943</td>\n",
       "      <td>-0.892044</td>\n",
       "      <td>-1.029242</td>\n",
       "      <td>-0.229461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118018</td>\n",
       "      <td>0.496910</td>\n",
       "      <td>-1.639284</td>\n",
       "      <td>-1.074811</td>\n",
       "      <td>1.134816</td>\n",
       "      <td>0.815747</td>\n",
       "      <td>-1.456636</td>\n",
       "      <td>-1.193091</td>\n",
       "      <td>-0.225801</td>\n",
       "      <td>-0.183995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.458755</td>\n",
       "      <td>1.339423</td>\n",
       "      <td>-0.016885</td>\n",
       "      <td>-1.097572</td>\n",
       "      <td>0.753981</td>\n",
       "      <td>-1.123875</td>\n",
       "      <td>0.599176</td>\n",
       "      <td>-0.063372</td>\n",
       "      <td>0.711301</td>\n",
       "      <td>-3.026947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175420</td>\n",
       "      <td>-0.878049</td>\n",
       "      <td>-0.701771</td>\n",
       "      <td>-0.996197</td>\n",
       "      <td>0.900043</td>\n",
       "      <td>-0.808168</td>\n",
       "      <td>-1.336955</td>\n",
       "      <td>-1.193091</td>\n",
       "      <td>0.507272</td>\n",
       "      <td>0.833438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.744762</td>\n",
       "      <td>-0.730045</td>\n",
       "      <td>0.326005</td>\n",
       "      <td>0.575556</td>\n",
       "      <td>1.121723</td>\n",
       "      <td>-0.301376</td>\n",
       "      <td>0.931104</td>\n",
       "      <td>-0.870237</td>\n",
       "      <td>0.307697</td>\n",
       "      <td>-2.327576</td>\n",
       "      <td>...</td>\n",
       "      <td>1.144822</td>\n",
       "      <td>-0.786385</td>\n",
       "      <td>-1.383599</td>\n",
       "      <td>-1.086844</td>\n",
       "      <td>0.401149</td>\n",
       "      <td>0.905964</td>\n",
       "      <td>-1.396796</td>\n",
       "      <td>0.103367</td>\n",
       "      <td>0.477949</td>\n",
       "      <td>-0.430320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  1.900803 -1.008627  1.673073 -0.882219  0.263659 -0.765146 -1.651706   \n",
       "1 -0.458755 -1.327007 -0.996571  0.592122  1.366884 -0.344978 -1.641333   \n",
       "2 -0.387254  0.145500  1.538366  0.790909 -1.575049 -0.413355  0.332943   \n",
       "3 -0.458755  1.339423 -0.016885 -1.097572  0.753981 -1.123875  0.599176   \n",
       "4 -0.744762 -0.730045  0.326005  0.575556  1.121723 -0.301376  0.931104   \n",
       "\n",
       "        7         8         9    ...       91        92        93        94   \\\n",
       "0 -1.044694  0.332922  1.502317  ...  1.259626 -2.069681  1.088027 -1.003417   \n",
       "1  1.354094 -1.004017  0.370001  ...  1.202224 -1.932185 -1.639284  0.760586   \n",
       "2 -0.892044 -1.029242 -0.229461  ... -0.118018  0.496910 -1.639284 -1.074811   \n",
       "3 -0.063372  0.711301 -3.026947  ... -0.175420 -0.878049 -0.701771 -0.996197   \n",
       "4 -0.870237  0.307697 -2.327576  ...  1.144822 -0.786385 -1.383599 -1.086844   \n",
       "\n",
       "        95        96        97        98        99        100  \n",
       "0  1.203292  1.221725 -0.379508 -1.199808 -2.073145  0.362206  \n",
       "1  1.213074  1.131508  0.996822 -1.139352  0.609902 -0.001928  \n",
       "2  1.134816  0.815747 -1.456636 -1.193091 -0.225801 -0.183995  \n",
       "3  0.900043 -0.808168 -1.336955 -1.193091  0.507272  0.833438  \n",
       "4  0.401149  0.905964 -1.396796  0.103367  0.477949 -0.430320  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = preprocessing.StandardScaler()\n",
    "X_standard_scaled = normalizer.fit_transform(X_no_mis)\n",
    "X_standard_scaled_pd = pd.DataFrame(data=X_standard_scaled)\n",
    "X_standard_scaled_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scaler = preprocessing.MinMaxScaler()\n",
    "X_mm_scaled = mm_scaler.fit_transform(X_no_mis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберем пример данных, когда масштабирование признаков сильно влияет на результат работы алгоритмов машинного обучения. Рассмотрим точки, равномерно нанесенные на плоскость (в данном датасете в качестве признаков x и y выступают координаты - долгота и широта точек, все точки территориально находятся в Москве)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File scaler_example.csv does not exist: 'scaler_example.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e894d52e85eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scaler_example.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File scaler_example.csv does not exist: 'scaler_example.csv'"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('scaler_example.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят точки из датасета. Точки относятся к двум классам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "Xtrain = X[['lat','lon']].values\n",
    "Ytrain = X['class'].values\n",
    "\n",
    "figure(figsize=(10,10))\n",
    "scatter(Xtrain[Ytrain==1][:,0], Xtrain[Ytrain==1][:,1], color='green')\n",
    "scatter(Xtrain[Ytrain==0][:,0], Xtrain[Ytrain==0][:,1], color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(Xtrain, Ytrain)\n",
    "\n",
    "def plot_decision_line(Xtrain, Ytrain, clf, h):\n",
    "\n",
    "    x_min, x_max = Xtrain[:,0].min() - 0.01, Xtrain[:,0].max() + 0.01\n",
    "    y_min, y_max = Xtrain[:,1].min() - 0.01, Xtrain[:,1].max() + 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.contourf(xx, yy, Z)\n",
    "\n",
    "    Ytrain = np.array(Ytrain)\n",
    "    plt.scatter(Xtrain[Ytrain==1][:,0], Xtrain[Ytrain==1][:,1], color='green')\n",
    "    plt.scatter(Xtrain[Ytrain==0][:,0], Xtrain[Ytrain==0][:,1], color='red')\n",
    "\n",
    "    a = clf.coef_[0][0]\n",
    "    b = clf.coef_[0][1]\n",
    "    c = clf.intercept_\n",
    "\n",
    "    K = -a * 1. / b\n",
    "    B = -c * 1. / b\n",
    "\n",
    "    xx0 = np.linspace(x_min, x_max)\n",
    "    yy0 = K * xx0 + B\n",
    "\n",
    "    plt.scatter(xx0, yy0)\n",
    "    plt.show()\n",
    "    \n",
    "plot_decision_line(Xtrain, Ytrain, clf, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Мы видим, что SVM абсолютно не справился с задачей. Попробуем теперь масштабировать данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "TrainScaled = sc.fit_transform(Xtrain)\n",
    "\n",
    "clf.fit(TrainScaled, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_line(TrainScaled, Ytrain, clf, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Работа с текстовыми признаками**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из направлений машинного обучения является работа с текстами и извлечение полезной информации из текстов. Чтобы алгоритмы машинного обучения могли работать с текстами, необходимо перевести тексты в наборы чисел. Для этого применяют различные алгоритмы векторизации текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем изучать датасет, содержащий отзывы о фильмах. Отзывы могут быть положительные, либо отрицательные. Наша конечная задача - научиться различать положительные и отрицательные отзывы.\n",
    "\n",
    "Загрузим датасет и уберем из него плохие строки (в которых нет оценки фильму)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open('imdb_labelled.txt', encoding='utf-8') as thefile:\n",
    "    print(thefile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "bad = 0\n",
    "with codecs.open('imdb_labelled.txt', encoding='utf-8') as thefile:\n",
    "    for row in tqdm(thefile.readlines()):\n",
    "        # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_responses = list(filter(lambda review: 'awful' in review, X))\n",
    "print(bad_responses[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bad_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые этапы обработки текста:\n",
    "\n",
    "* приведение к нижнему регистру\n",
    "\n",
    "* удаление пунктуации\n",
    "\n",
    "* удаление всех символов, кроме символов нашего алфавита (в данном случае, латинского)\n",
    "\n",
    "Удалить пунктуацию можно при помощи регулярных выражений. [Прикольный сайт для создания регулярок](https://regex101.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(r'[^\\w\\s]', '', bad_responses[1].lower()).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = [re.sub(r'[^\\w\\s]', '', elem.lower()).strip() for elem in X]\n",
    "Texts[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение ответов в наших данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 способ векторизации: счётчик (CountVectorizer)**\n",
    "\n",
    "Каждому слову соответствует количество его вхождений в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(Texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.transform(Texts[:1]).indptr)\n",
    "print(vectorizer.transform(Texts[:1]).indices)\n",
    "print(vectorizer.transform(Texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 способ векторизации: TF-IDF**\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — TF-IDF (Term Frequency–Inverse Document Frequency). Рассмотрим коллекцию текстов $D$. Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте: \n",
    "    $$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$ где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "2. Inverse Document Frequency $$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$ где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину: $$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n",
    "\n",
    "Признаковым описанием одного объекта $d \\in D$ будет вектор $\\bigg(\\text{tf-idf}(t,d, D)\\bigg)_{t\\in V}$, где $V$ – словарь всех слов, встречающихся в коллекции $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(Texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.transform(Texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.transform(Texts[:1]).indptr)\n",
    "print(vectorizer.transform(Texts[:1]).indices)\n",
    "print(vectorizer.transform(Texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим два рассмотренных метода векторизации к задаче классификации отзывов на два класса (положительные и отрицательные)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(Texts)\n",
    "\n",
    "X = vectorizer.transform(Texts)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Важность признаков**\n",
    "\n",
    "В задачах, связанных с обработкой текстов, признаки как правило хорошо интерпретируемы. Для визуального контроля качества работы алгоритма можно посмотреть на те слова, которые алгоритм посчитал наиболее важными для данной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "weights = sorted(weights, key=lambda i: i[1])\n",
    "for i in range(1,20):\n",
    "    print('%s, %.2f' % weights[-i])\n",
    "    \n",
    "print('...')\n",
    "for i in reversed(range(1,20)):\n",
    "    print('%s, %.2f' % weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3 способ векторизации: Word2Vec**\n",
    "\n",
    "Word2Vec - это алгоритм, который собирает статистику по совместному появлению слов в фразах, а затем с помощью нейронных сетей решает задачу снижения размерности и выдает на выходе компактные векторные представления слов, в максимальной степени отражающие отношения этих слов в обрабатываемых текстах.\n",
    "\n",
    "Нахождение связей между контекстами слов основано на предположении, что слова, находящиеся в похожих контекстах, имеют тенденцию значить похожие вещи, т.е. быть семантически близкими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим векторизацию с помощью word2vec для наших данных. Кроме того, удалим stop-слова, то есть слова, часто встречающиеся во всех английских текстах - это ещё один полезный метод обработки текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_stopwords(review, remove_stopwords=True):\n",
    "    \n",
    "    words = review.split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переведем строки нашего датасета в токенизированный вид и удалим из них стоп-слова - в этом виде они пригодны для использования word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def review_to_sentences(review,tokenizer,remove_stopwords=True ):\n",
    "\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    sentences = []\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(delete_stopwords(sentence,remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "sentences = []\n",
    "Y = []\n",
    "for i in range(len(Texts)):\n",
    "    if len(set(Texts[i])) == 1:\n",
    "        continue\n",
    "    Y.append(y[i])\n",
    "    sentences += review_to_sentences(Texts[i], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим word2vec к токенизированному корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 512         \n",
    "min_word_count = 3           \n",
    "num_workers = 4       \n",
    "context = 5                                                                          \n",
    "downsampling = 1e-5   \n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=42)\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = str(num_features) + \"features_word2vec\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь каждое слово корпуса имеет векторное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.wv['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве одного из способов векторизовать текст (в нашем случае отзыв на фильм), можно усреднить векторы слов, входящих в этот текст. Так и сделаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_set = set(model.wv.index2word)\n",
    "\n",
    "def normalize(x):\n",
    "    return x / np.sqrt(np.dot(x, x))\n",
    "\n",
    "def make_featurevec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            featureVec += model[word]\n",
    "    if nwords > 0:\n",
    "        featureVec = normalize(featureVec)\n",
    "        # featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "def get_avg_featurevecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = make_featurevec(review, model, num_features)\n",
    "        counter += 1\n",
    "    return reviewFeatureVecs\n",
    "\n",
    "trainDataVecs = get_avg_featurevecs(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, обучим классификатор на полученных признаках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(trainDataVecs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict_proba(X_test)[:,1]\n",
    "print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(y_test, preds), accuracy_score(y_test, (preds > 0.5).astype(int))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
